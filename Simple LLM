import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from transformers import GPT2Tokenizer
import torch.distributed as dist
from torch.cuda.amp import GradScaler, autocast

# ------------
# 1. Distributed Training Setup
# ------------
def setup_distributed():
    dist.init_process_group(backend='nccl')
    torch.cuda.set_device(int(os.environ['LOCAL_RANK']))

# ------------
# 2. Model Architecture (Simplified GPT-3)
# ------------
class TransformerBlock(nn.Module):
    def __init__(self, d_model=1024, n_heads=16):
        super().__init__()
        self.attn = nn.MultiheadAttention(d_model, n_heads)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, 4*d_model),
            nn.GELU(),
            nn.Linear(4*d_model, d_model)
        )
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

    def forward(self, x):
        x = self.norm1(x + self.attn(x, x, x)[0])
        x = self.norm2(x + self.ffn(x))
        return x

class LLM(nn.Module):
    def __init__(self, vocab_size=50257, n_layers=24, d_model=1024):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, d_model)
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(d_model) for _ in range(n_layers)
        ])
        self.fc = nn.Linear(d_model, vocab_size)

    def forward(self, x):
        x = self.embed(x)
        for block in self.transformer_blocks:
            x = block(x)
        return self.fc(x)

# ------------
# 3. Training Infrastructure
# ------------
class LLMTrainer:
    def __init__(self, model, tokenizer, train_data, batch_size=32):
        self.model = DDP(model.cuda())
        self.optimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)
        self.scaler = GradScaler()
        self.sampler = DistributedSampler(train_data)
        self.loader = DataLoader(train_data, batch_size=batch_size, sampler=self.sampler)
        self.tokenizer = tokenizer

    def train_step(self, batch):
        inputs, targets = batch
        with autocast():
            outputs = self.model(inputs.cuda())
            loss = nn.CrossEntropyLoss()(outputs.view(-1, outputs.size(-1)), targets.view(-1).cuda())
        
        self.scaler.scale(loss).backward()
        self.scaler.step(self.optimizer)
        self.scaler.update()
        self.optimizer.zero_grad()
        return loss.item()

# ------------
# 4. Data Pipeline (Simplified)
# ------------
class TextDataset(Dataset):
    def __init__(self, texts, tokenizer, seq_len=512):
        self.tokens = [tokenizer.encode(text)[:seq_len] for text in texts]

    def __getitem__(self, idx):
        x = torch.tensor(self.tokens[idx][:-1], dtype=torch.long)
        y = torch.tensor(self.tokens[idx][1:], dtype=torch.long)
        return x, y

# ------------
# 5. Execution
# ------------
if __name__ == "__main__":
    # Initialize distributed training
    setup_distributed()
    
    # Load data and tokenizer
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    dataset = TextDataset(your_text_data, tokenizer)  # Replace with actual data
    
    # Initialize model and trainer
    model = LLM(vocab_size=tokenizer.vocab_size)
    trainer = LLMTrainer(model, tokenizer, dataset)
    
    # Training loop
    for epoch in range(10):
        for batch in trainer.loader:
            loss = trainer.train_step(batch)
            print(f"Loss: {loss:.4f}")
